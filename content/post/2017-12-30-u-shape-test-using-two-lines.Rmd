---
title: U-Shape Test using "Two Lines"
author: Xing
date: '2017-12-30'
output:
  blogdown::html_page:
    toc: true
slug: u-shape-test-using-two-lines
categories: []
tags:
  - Statistics
  - Clustered Standard Error
  - Econometrics
  - Data
  - Model Selection
---

This post shows how to use Two-lines method to test U-shape relationship, and how to estimate the model with clustered standard errors and many fixed effects.

# Motivation
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
In this [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3021690) by Uri Simonsohn (2017), the author proposed a noval method to test U-Shape relationship. In the literature, the popular way of testing U-shapeness relationship between `x` and `y` is to add a quadratic term in the regression $y=\beta_0+\beta_1 x + \beta_2 x^2 +\epsilon$ ($\epsilon$ is an *i.i.d* noise). If $\beta_1$ is statistitally significant, then the relationship betewen `x` and `y` are U-shape. There are plenty of examples in the real world that two variables have such kind of relationship (consider x = amount of sugar in the icecream, and y = the taste). Simonsohn pointed out that using the quadratic regression would lead to high false positive rate.

I am directly borrowing the example he gave in the paper. Suppose the underlying relationship is $y=\log{x} +\epsilon$, where $x\sim U[0,1]$. We first simulate the data and have the following plot:


```{r Results}
set.seed(111)
obs = 10000 # Sample Size of Observations
x = runif(n = obs)^2 # Unnecessary to have the ^2 term, 
# but it gives more striking graph fitting the quadratic term
x2 = x*x
y=log(x)+ rnorm(obs, 0, 1)
plot(x, y)
curve(log(x),from=0,to=1,col='red',
      lwd='2',ylab="y",xlab="x",xaxt='n',add=TRUE)
```

The yellow line is the curve $y=\log{x}$, and using naked eyes, there is no way it would be close to a U-shape. Now let's fit a quadratic model  $y=\beta_0+\beta_1 x + \beta_2 x^2 +\epsilon$ in the regression, and we get:

```{r regression}
summary(lm(y~x+x2))
```
From the the results, the coefficients on both $x$ and $x^2$ are highly significant. In the plot, we have:

```{r Results1, echo = FALSE}
plot(x, fitted(lm(y~x+x2)),col='black',ylab='y')
curve(log(x),from=0,to=1,col='red',
      lwd='2',ylab="y",xlab="x",xaxt='n',add=TRUE)
legend(.5,-3,col=c('red','black'),lty=c(1,2),lwd=2,
       c("True relationship:        y=log(x)",
         expression(paste("Quadratic regression: y = 14.1x - 10.57",x^2, sep = ""))),cex=.65)
text(0.2,0,"The true model and fitted model",cex=1)
```
In such case, the regression with quadratic term may not be a good choice to test the underlying relationship between `x` and `y`. How to fix it? 

# Two-lines Approaches

The idea behind Professor Simonsohn's two-lines approach is rather simple and intuitive: if `x` and `y` truly have U-shape relationship (in the example, it is an inverted U-shape), we should be able to find a cutoff point where `x` and `y` should have opposite relationship before and after the point. 

Consider the example above. When we fitted the quadratic curve, it implies that if $x <\frac{14.1}{2 * 10.57}=0.67$, the axis of symmetry, $x$ has a positive relationship with $y$, and if $x >0.67$, $x$ has a negative relationship with $y$, which we know from the data generating process is untrue. Intuitively (the intuition is going to be revisited shortly), we can simply run two linear regressions:

* $y=\beta_0+\beta_1^{1}\cdot x+\epsilon$, when $x<0.67$
* $y=\beta_0+\beta_1^{2}\cdot x+\epsilon$, when $x>0.67$

If $\beta_1^{1}$ is positive and $\beta_1^{2}$ is negative, and both coefficients are significant, then we can be more confident that we find an inverted U-shape relationship.

Interestingly, as the author shows in the paper, the quadratic function's axis of symmetry is not a good option to serve as the cut-off point. He proposed a Robinhood algorithm which would automatically find the cut-off point. In a horse-race simulation test, it gives lowest type-I error rate and highest power. 


# Regression with Clustered Standard Errors and Many Fixed Effects

The author provides an [online App](http://webstimate.org/twolines/) and corresponding [R code](http://webstimate.org/twolines/twolines_2017_11_28.R) in which other researchers can simply upload the data and conduct two-lines test by themselves. When I was trying to use the method, I realized that there several issues that the code cannot directly apply to my data. In my own work, I have:

* A bunch of fixed effects I have to control. My economists friends usually have hundreds or thousands fixed effects to control (like controlling each counties' specific effect in the US.)
* The standard errors are serially correlated within certain segments

Hence I break the code so that it can adress this two issues in my own work. Since I was a kid, I enjoyed breaking toys and seeing how things work. In rare case, I could break them and improve the functionality. 


